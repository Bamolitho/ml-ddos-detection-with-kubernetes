# ============================================================
# Configuration globale des expériences
# ============================================================

models:
  # Tree-Based Models
  # Valeurs possibles : true | false
  decision_tree: false
  random_forest: false
  xgboost: true
  lightgbm: true
  catboost: false
  gradient_boosting: false
  adaboost: false
  
  # Linear Models
  # Valeurs possibles : true | false
  logistic_regression: false
  svm: false

  # Simple Models
  # Valeurs possibles : true | false
  naive_bayes: false
  knn: false


# ============================================================
# Configuration des hyperparamètres
# ============================================================

hyperparameters:

  # ---------------- Tree-Based ----------------

  decision_tree:
    # criterion : "gini" | "entropy" | "log_loss"
    criterion: "gini"

    # splitter : "best" | "random"
    splitter: "best"

    # max_depth : int | null
    max_depth: null

    # min_samples_split : int >= 2
    min_samples_split: 2

    # min_samples_leaf : int >= 1
    min_samples_leaf: 1

    # max_features : "auto" | "sqrt" | "log2" | int | float | null
    max_features: null


  random_forest:
    # n_estimators : int >= 1
    n_estimators: 300

    # criterion : "gini" | "entropy" | "log_loss"
    criterion: "gini"

    # max_depth : int | null
    max_depth: null

    # min_samples_split : int >= 2
    min_samples_split: 2

    # min_samples_leaf : int >= 1
    min_samples_leaf: 1

    # max_features : "auto" | "sqrt" | "log2" | int | float | null
    max_features: "auto"

    # bootstrap : true | false
    bootstrap: true

    # n_jobs : -1 = utiliser tous les coeurs
    n_jobs: -1


  gradient_boosting:
    # loss : "log_loss" | "deviance" | "exponential"
    loss: "log_loss"

    # learning_rate : float
    learning_rate: 0.05

    # n_estimators : int
    n_estimators: 200

    # subsample : float entre 0.0 et 1.0
    subsample: 1.0

    # criterion : "friedman_mse" | "squared_error"
    criterion: "friedman_mse"

    # max_depth : int
    max_depth: 3


  adaboost:
    # n_estimators : int
    n_estimators: 200

    # learning_rate : float
    learning_rate: 0.5


  xgboost:
    # n_estimators : int
    n_estimators: 500

    # max_depth : int
    max_depth: 8

    # learning_rate : float
    learning_rate: 0.1

    # subsample : float (0.0 - 1.0)
    subsample: 0.8

    # colsample_bytree : float (0.0 - 1.0)
    colsample_bytree: 0.8

    # objective : "binary:logistic" | "binary:logitraw" | "binary:hinge"
    objective: "binary:logistic"

    # eval_metric : "logloss" | "auc" | "aucpr" | "error"
    eval_metric: "logloss"

    # tree_method : "auto" | "hist" | "gpu_hist" | "approx"
    tree_method: "hist"

    # n_jobs : -1 = tous les coeurs
    n_jobs: -1


  lightgbm:
    # boosting_type : "gbdt" | "dart" | "rf"
    boosting_type: "gbdt"

    # num_leaves : int (puissance de 2 recommandée)
    num_leaves: 256

    # learning_rate : float
    learning_rate: 0.05

    # n_estimators : int
    n_estimators: 500

    # max_depth : -1 = illimité
    max_depth: -1

    # subsample : float
    subsample: 0.8

    # colsample_bytree : float
    colsample_bytree: 0.8

    # objective : "binary" | "cross_entropy" | "cross_entropy_lambda"
    objective: "binary"

    # n_jobs : -1 = tous les coeurs
    n_jobs: -1


  catboost:
    # iterations : int
    iterations: 500

    # depth : int (4-10 recommandé)
    depth: 8

    # learning_rate : float
    learning_rate: 0.1

    # loss_function : "Logloss" | "CrossEntropy"
    loss_function: "Logloss"

    # task_type : "CPU" | "GPU"
    task_type: "CPU"

    # thread_count : -1 = tous les coeurs
    thread_count: -1


  # ---------------- Linear Models ----------------

  logistic_regression:
    # penalty : "l1" | "l2" | "elasticnet" | "none"
    penalty: "l2"

    # C : float > 0
    C: 1.0

    # solver : "lbfgs" | "liblinear" | "newton-cg" | "sag" | "saga"
    solver: "lbfgs"

    # max_iter : int
    max_iter: 500

    # class_weight : "balanced" | null | dict
    class_weight: null


  svm:
    # kernel : "linear" | "poly" | "rbf" | "sigmoid"
    kernel: "rbf"

    # C : float > 0
    C: 1.0

    # gamma : "scale" | "auto" | float
    gamma: "scale"

    # probability : true | false
    probability: true

    # class_weight : "balanced" | null | dict
    class_weight: null


  # ---------------- Simple Models ----------------

  naive_bayes:
    # var_smoothing : float
    var_smoothing: 1e-9


  knn:
    # n_neighbors : int
    n_neighbors: 5

    # weights : "uniform" | "distance"
    weights: "uniform"

    # algorithm : "auto" | "ball_tree" | "kd_tree" | "brute"
    algorithm: "auto"

    # leaf_size : int
    leaf_size: 30

    # p : 1 = Manhattan | 2 = Euclidean
    p: 2


# ============================================================
# Configuration du déséquilibre du dataset
# ============================================================

imbalance:
  # strategy : none | oversample | undersample | smote | class_weight | hybrid (over/under)
  strategy: "class_weight"

  # oversample_method : random | smote
  oversample_method: "random"

  # undersample_method : random
  undersample_method: "random"

  # undersample_ratio : float entre 0.0 et 1.0
  undersample_ratio: 0.5

  # oversample_ratio : float >= 1.0
  oversample_ratio: 1.0

  # auto_class_weight : true | false
  auto_class_weight: true
