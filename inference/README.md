# Inference ‚Äî Pr√©diction DDoS (ML)

Ce module est responsable de **l‚Äôinf√©rence machine learning**.

Il prend des features r√©seau en entr√©e, applique le pipeline de pr√©traitement,
ex√©cute le mod√®le ML, et retourne une d√©cision exploitable par l‚Äôorchestrateur.

Il ne capture rien, ne bloque rien, ne stocke rien.
üëâ Il **pr√©dit uniquement**.

---

## R√¥le du module

Le module `inference` a un seul objectif :

‚û°Ô∏è transformer des features r√©seau en **verdict ML fiable**.

Il fournit :
- une pr√©diction binaire (0 / 1)
- une probabilit√©
- un verdict lisible (`Benign` / `DDoS`)
- un seuil optimis√© automatiquement

---

## Structure du r√©pertoire

```bash
inference/
‚îú‚îÄ‚îÄ predict.py # Script principal d‚Äôinf√©rence
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ README.md
```

---

## Point d‚Äôentr√©e principal

Le script est con√ßu pour √™tre appel√© :
- en **CLI**
- par un **autre module Python** (orchestrator)

Commande typique :

```bash
python inference/predict.py --json "<features_json>"
```

------

## Principe de fonctionnement

### 1. Chargement des composants

Au d√©marrage, le script charge dynamiquement :

- le **pipeline de pr√©traitement**
- le **mod√®le ML**
- le **seuil optimal** calcul√© lors de l‚Äô√©valuation

Sources :

| √âl√©ment  | Emplacement                                |
| -------- | ------------------------------------------ |
| Mod√®le   | `models/<model_name>.pkl`                  |
| Pipeline | `data/processed/preprocessed_pipeline.pkl` |
| Seuil    | `evaluate/benchmark.csv`                   |

------

### 2. Gestion du seuil de d√©cision

Le seuil n‚Äôest **pas cod√© en dur**.

Il est charg√© depuis `benchmark.csv`, g√©n√©r√© lors de la phase d‚Äô√©valuation :

```
model,best_threshold
xgboost,0.11
```

‚û°Ô∏è Cela permet d‚Äôadapter la d√©cision au mod√®le sans modifier le code.

------

### 3. Pr√©traitement des donn√©es

Les donn√©es d‚Äôentr√©e sont transform√©es via le pipeline :

```python
X = pipeline.transform(data)
```

Le pipeline garantit :

- ordre correct des features
- normalisation
- encodage
- compatibilit√© exacte avec l‚Äôentra√Ænement

------

### 4. Pr√©diction

Pour chaque flow :

- calcul de la probabilit√© (`predict_proba`)
- comparaison avec le seuil optimal
- conversion en verdict final

R√®gle :

```
probability >= threshold ‚Üí DDoS
sinon ‚Üí Benign
```

------

## Formats d‚Äôentr√©e support√©s

### 1. JSON (utilis√© en production)

```bash
--json '{"Flow Duration":123,"Total Fwd Packets":10,...}'
```

C‚Äôest le mode utilis√© par l‚Äôorchestrator Kubernetes.

------

### 2. CSV (tests / batch)

```bash
--input data/processed/test.csv
```

Optionnellement accompagn√© de labels pour l‚Äô√©valuation.

------

## Format de sortie (JSON)

Sortie unique sur `stdout` :

```json
{
  "model": "xgboost",
  "threshold": 0.11,
  "results": [
    {
      "prediction": 1,
      "probability": 0.92,
      "verdict": "DDoS"
    }
  ]
}
```

Ce format est **consomm√© directement** par l‚Äôorchestrator.

------

## Support des m√©triques (optionnel)

Si un fichier de labels est fourni :

- le script g√©n√®re automatiquement un `classification_report`
- les m√©triques sont ajout√©es au JSON de sortie

Utile pour :

- validation
- tests hors production
- benchmark

------

## S√©lection du mod√®le

Priorit√© de s√©lection :

1. `--model <nom>` (CLI)
2. `config/config_inference.yaml` (si activ√©)
3. mod√®le par d√©faut : `xgboost`

Aucun changement de code n‚Äôest n√©cessaire pour changer de mod√®le.

------

## Ce que ce module NE fait PAS

- pas de capture r√©seau
- pas de base de donn√©es
- pas de SOAR
- pas de r√®gles de s√©curit√©
- pas de logique m√©tier

üëâ Il reste **volontairement simple et isol√©**.

------

## Int√©gration dans l‚Äôarchitecture globale

```
Features r√©seau
      ‚Üì
Preprocessing pipeline
      ‚Üì
Mod√®le ML
      ‚Üì
Probabilit√© + verdict
      ‚Üì
Orchestrator
```

------

## Avantages de cette approche

- d√©couplage total du ML
- mod√®le interchangeable
- seuil ajustable sans code
- testable en standalone
- reproductible

------

## Perspectives d‚Äô√©volution

- support multi-mod√®les simultan√©s
- ensemble learning
- calibration dynamique des seuils
- export ONNX
- acc√©l√©ration GPU
- d√©tection multi-classes

------

## R√©sum√©

Le module `inference` est le **moteur de d√©cision statistique** du projet.

Il transforme des statistiques r√©seau en une **pr√©diction exploitable**,
sans d√©pendance au contexte r√©seau ou s√©curit√©.

Simple, fiable, et pr√™t pour la production.