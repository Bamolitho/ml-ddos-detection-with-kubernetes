# preprocessed_data ‚Äî Pr√©processing et √©quilibrage des donn√©es

Ce r√©pertoire contient **toute la logique de pr√©paration des donn√©es avant l‚Äôentra√Ænement ML**.

Objectifs principaux :
- transformer le dataset brut en donn√©es exploitables
- √©quilibrer le dataset DDoS / BENIGN
- supprimer les informations inutiles ou dangereuses
- produire des splits reproductibles
- sauvegarder un pipeline de preprocessing r√©utilisable en production

C‚Äôest une **brique critique du pipeline ML**.

---

## Structure du r√©pertoire

```bash
preprocessed_data/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ preprocessed_data.py
‚îú‚îÄ‚îÄ sampling.py
‚îú‚îÄ‚îÄ sampling_v2.py
‚îú‚îÄ‚îÄ preprocessing_pipeline/
‚îÇ ‚îú‚îÄ‚îÄ preprocessing_pipeline.py
‚îÇ ‚îú‚îÄ‚îÄ config.yaml
‚îÇ ‚îú‚îÄ‚îÄ main.py
‚îÇ ‚îú‚îÄ‚îÄ README.md
‚îÇ ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## Vue d‚Äôensemble du pipeline

1. Chargement du dataset brut
2. Conversion en classification **binaire** (BENIGN / DDoS)
3. √âquilibrage du dataset
4. Nettoyage des colonnes
5. Splits train / validation / test
6. Fit du preprocessing **uniquement sur train**
7. Transformation coh√©rente de val et test
8. Sauvegarde :
   - des datasets
   - du pipeline de preprocessing

---

## Choix fondamentaux de conception

### Classification binaire

- **0** ‚Üí trafic BENIGN  
- **1** ‚Üí trafic DDoS (toutes attaques confondues)

Raison :
- plus robuste
- plus simple √† d√©ployer
- plus r√©aliste en production

Un IDS orient√© DDoS doit r√©pondre √† **une seule question** :

> *Ce trafic est-il normal ou malveillant ?*

---

## Politique de gestion des colonnes

### Colonnes supprim√©es volontairement

| Colonne          | D√©cision  | Raison                                  |
| ---------------- | --------- | --------------------------------------- |
| `Flow ID`        | supprim√©e | Identifiant sans valeur ML              |
| `Source IP`      | supprim√©e | Apprentissage biais√©, non g√©n√©ralisable |
| `Destination IP` | supprim√©e | M√™me raison                             |
| `Timestamp`      | supprim√©e | Date brute non exploitable              |
| `Unnamed: 0`     | supprim√©e | Index pandas                            |
| `SimillarHTTP`   | supprim√©e | Bruit et faible valeur                  |

### Colonnes conserv√©es

- dur√©es de flux
- tailles de paquets
- IAT
- compteurs
- flags TCP
- statistiques (mean, std, variance, max, min)

üëâ Ce sont **les signaux cl√©s en d√©tection DDoS**.

---

## Scripts principaux

### preprocessed_data.py

Script central de preprocessing.

Responsabilit√©s :
- chargement du dataset √©quilibr√©
- nettoyage brut (NaN, inf)
- suppression des colonnes inutiles
- splits stratifi√©s :
  - 60 % train
  - 20 % validation
  - 20 % test
- suppression des doublons **uniquement dans train**
- fit du pipeline uniquement sur train
- transformation coh√©rente de val et test
- sauvegarde des fichiers et du pipeline

Sorties g√©n√©r√©es dans `data/processed/` :
- `train.csv`, `val.csv`, `test.csv`
- `train_processed.csv`, `val_processed.csv`, `test_processed.csv`
- `train_labels.csv`, `val_labels.csv`, `test_labels.csv`
- `preprocessed_pipeline.pkl`

---

### sampling.py

Script avanc√© d‚Äô√©quilibrage du dataset.

Fonctionnement :
- lecture du dataset par chunks (RAM-safe)
- regroupement par type d‚Äôattaque
- conversion robuste en binaire
- sous-√©chantillonnage dynamique des attaques
- conservation contr√¥l√©e du trafic BENIGN
- √©quilibrage final via **SMOTEENN**
- gestion des cas extr√™mes (classes rares)

Objectif :
- obtenir un dataset √©quilibr√© **sans d√©former la distribution r√©elle**

---

### sampling_v2.py

Version alternative plus simple et plus rapide.

Approche :
- streaming du dataset
- conservation totale de BENIGN
- sous-√©chantillonnage al√©atoire des DDoS
- √©quilibrage final par undersampling simple

Utilisation :
- tests rapides
- environnements contraints
- validation exploratoire

---

## preprocessing_pipeline/

Contient le **pipeline de transformation des features**.

### preprocessing_pipeline.py

Pipeline maison bas√© sur scikit-learn.

Fonctions principales :
- suppression des features √† faible variance
- suppression des features trop corr√©l√©es
- encodage du protocole
- gestion des flags r√©seau
- normalisation des features num√©riques

---

### config.yaml

Fichier de configuration du preprocessing.

Permet d‚Äôactiver/d√©sactiver :
- normalisation
- encodage cat√©goriel
- filtrage statistique
- r√©duction de dimension
- s√©lection de features

Les param√®tres sont **centralis√©s et reproductibles**.

---

## Bonnes pratiques appliqu√©es

- aucun leakage train ‚Üí val ‚Üí test
- pipeline appris uniquement sur train
- m√™mes transformations appliqu√©es partout
- preprocessing s√©rialis√© pour l‚Äôinf√©rence
- scripts reproductibles
- gestion m√©moire adapt√©e aux gros datasets

---

## Quand utiliser ce dossier

- pr√©paration initiale des donn√©es
- re-g√©n√©ration des datasets
- changement de strat√©gie de sampling
- mise √† jour du preprocessing
- alignement entra√Ænement ‚Üî production

---

## Perspectives d‚Äô√©volution

- automatisation du choix de sampling
- monitoring du d√©s√©quilibre r√©el en production
- d√©tection de d√©rive des features
- feature selection adaptative
- int√©gration directe avec MLflow / DVC

---

## R√©sum√©

Le dossier `preprocessed_data/` transforme un dataset r√©seau brut et massif en **donn√©es propres, √©quilibr√©es et exploitables**, pr√™tes pour l‚Äôentra√Ænement et l‚Äôinf√©rence d‚Äôun syst√®me de d√©tection DDoS en production.
